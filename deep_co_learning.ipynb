{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOFl+f1m70S+7TItFd2m9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thotakuria/surgical-tool-segmentation-using-deep-learning-techniques/blob/main/deep_co_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = transforms.Compose([\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor()])\n",
        "image_datasets = datasets.ImageFolder(root= \"/content/gdrive/My Drive\", transform=data_transforms)\n",
        "dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "CdwhTPD2tIZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import sys, os\n",
        "import random\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "class CNN13(nn.Module):\n",
        "       \n",
        "      def __init__(self, num_classes=10, dropout=0.5):\n",
        "            super(CNN13, self).__init__()\n",
        "            self.activation = nn.LeakyReLU(0.1)\n",
        "            self.conv1a = weight_norm(nn.Conv2d(3, 128, 3, padding=1))\n",
        "            self.bn1a = nn.BatchNorm2d(128)\n",
        "            self.conv1b = weight_norm(nn.Conv2d(128, 128, 3, padding=1))\n",
        "            self.bn1b = nn.BatchNorm2d(128)\n",
        "            self.conv1c = weight_norm(nn.Conv2d(128, 128, 3, padding=1))\n",
        "            self.bn1c = nn.BatchNorm2d(128)\n",
        "            self.mp1 = nn.MaxPool2d(2, stride=2, padding=0)\n",
        "            self.drop1  = nn.Dropout(dropout)\n",
        "\n",
        "            self.conv2a = weight_norm(nn.Conv2d(128, 256, 3, padding=1))\n",
        "            self.bn2a = nn.BatchNorm2d(256)\n",
        "            self.conv2b = weight_norm(nn.Conv2d(256, 256, 3, padding=1))\n",
        "            self.bn2b = nn.BatchNorm2d(256)\n",
        "            self.conv2c = weight_norm(nn.Conv2d(256, 256, 3, padding=1))\n",
        "            self.bn2c = nn.BatchNorm2d(256)\n",
        "            self.mp2 = nn.MaxPool2d(2, stride=2, padding=0)\n",
        "            self.drop2  = nn.Dropout(dropout)\n",
        "\n",
        "            self.conv3a = weight_norm(nn.Conv2d(256, 512, 3, padding=0))\n",
        "            self.bn3a = nn.BatchNorm2d(512)\n",
        "            self.conv3b = weight_norm(nn.Conv2d(512, 256, 1, padding=0))\n",
        "            self.bn3b = nn.BatchNorm2d(256)\n",
        "            self.conv3c = weight_norm(nn.Conv2d(256, 128, 1, padding=0))\n",
        "            self.bn3c = nn.BatchNorm2d(128)\n",
        "            self.ap3 = nn.AvgPool2d(6, stride=2, padding=0)\n",
        "\n",
        "            self.fc1 =  weight_norm(nn.Linear(128, num_classes))\n",
        "        \n",
        "      def forward(self, x):\n",
        "\n",
        "            out = x\n",
        "            ## layer 1-a###\n",
        "            out = self.conv1a(out)\n",
        "            out = self.bn1a(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "            ## layer 1-b###\n",
        "            out = self.conv1b(out)\n",
        "            out = self.bn1b(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "            ## layer 1-c###\n",
        "            out = self.conv1c(out)\n",
        "            out = self.bn1c(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "            out = self.mp1(out)\n",
        "            out = self.drop1(out)\n",
        "\n",
        "\n",
        "            ## layer 2-a###\n",
        "            out = self.conv2a(out)\n",
        "            out = self.bn2a(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "            ## layer 2-b###\n",
        "            out = self.conv2b(out)\n",
        "            out = self.bn2b(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "            ## layer 2-c###\n",
        "            out = self.conv2c(out)\n",
        "            out = self.bn2c(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "\n",
        "            out = self.mp2(out)\n",
        "            out = self.drop2(out)\n",
        "\n",
        "\n",
        "            ## layer 3-a###\n",
        "            out = self.conv3a(out)\n",
        "            out = self.bn3a(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "            ## layer 3-b###\n",
        "            out = self.conv3b(out)\n",
        "            out = self.bn3b(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "            ## layer 3-c###\n",
        "            out = self.conv3c(out)\n",
        "            out = self.bn3c(out)\n",
        "            out = self.activation(out)\n",
        "\n",
        "            out = self.ap3(out)\n",
        "\n",
        "            out = out.view(-1, 128)\n",
        "            out = self.fc1(out)\n",
        "            return out\n",
        "def co_train_classifier(num_classes=10, dropout = 0.0):\n",
        "      model = CNN13(num_classes = num_classes, dropout=dropout)\n",
        "      return model"
      ],
      "metadata": {
        "id": "ntsNWtIwswbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "from tensorboardX import SummaryWriter "
      ],
      "metadata": {
        "id": "LlCtBuXRtIW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import argparse\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "!pip install tensorboardX\n",
        "from tensorboardX import SummaryWriter \n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "parser = argparse.ArgumentParser(description='Deep Co-Training for Semi-Supervised Image Recognition')\n",
        "parser.add_argument('--sess', default='default', type=str, help='session id')\n",
        "parser.add_argument('--batchsize', '-b', default=100, type=int)\n",
        "parser.add_argument('--lambda_cot_max', default=10, type=int)\n",
        "parser.add_argument('--lambda_diff_max', default=0.5, type=float)\n",
        "parser.add_argument('--seed', default=1234, type=int)\n",
        "parser.add_argument('--epochs', default=600, type=int)\n",
        "parser.add_argument('--warm_up', default=80.0, type=float)\n",
        "parser.add_argument('--momentum', default=0.9, type=float)\n",
        "parser.add_argument('--decay', default=1e-4, type=float)\n",
        "parser.add_argument('--epsilon', default=0.02, type=float)\n",
        "parser.add_argument('--num_class', default=10, type=int)\n",
        "parser.add_argument('--tensorboard_dir', default='tensorboard/', type=str)\n",
        "parser.add_argument('--checkpoint_dir', default='checkpoint', type=str)\n",
        "parser.add_argument('--base_lr', default=0.05, type=float)\n",
        "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "# for reproducibility\n",
        "seed = args.seed\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic=True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "torch.set_printoptions(precision=4)\n",
        "\n",
        "\n",
        "if not os.path.isdir(args.tensorboard_dir):\n",
        "    os.mkdir(args.tensorboard_dir)\n",
        "\n",
        "writer = SummaryWriter(args.tensorboard_dir)\n",
        "start_epoch = 0\n",
        "end_epoch = args.epochs\n",
        "class_num = args.num_class \n",
        "batch_size = args.batchsize\n",
        "lambda_cot_max = args.lambda_cot_max\n",
        "lambda_diff_max = args.lambda_diff_max\n",
        "lambda_cot = 0.0\n",
        "lambda_diff = 0.0\n",
        "best_acc = 0.0  \n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"cosine scheduling\"\"\"\n",
        "    epoch = epoch + 1\n",
        "    lr = args.base_lr*(1.0 + math.cos((epoch-1)*math.pi/args.epochs))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def adjust_lamda(epoch):\n",
        "    epoch = epoch + 1\n",
        "    global lambda_cot\n",
        "    global lambda_diff\n",
        "    if epoch <= args.warm_up:\n",
        "        lambda_cot = lambda_cot_max*math.exp(-5*(1-epoch/args.warm_up)**2)\n",
        "        lambda_diff = lambda_diff_max*math.exp(-5*(1-epoch/args.warm_up)**2)\n",
        "    else: \n",
        "        lambda_cot = lambda_cot_max\n",
        "        lambda_diff = lambda_diff_max   \n",
        "\n",
        "def loss_sup(logit_S1, logit_S2, labels_S1, labels_S2):\n",
        "    ce = nn.CrossEntropyLoss() \n",
        "    loss1 = ce(logit_S1, labels_S1)\n",
        "    loss2 = ce(logit_S2, labels_S2) \n",
        "    return (loss1+loss2)\n",
        "\n",
        "def loss_cot(U_p1, U_p2):)\n",
        "    S = nn.Softmax(dim = 1)\n",
        "    LS = nn.LogSoftmax(dim = 1)\n",
        "    a1 = 0.5 * (S(U_p1) + S(U_p2))\n",
        "    loss1 = a1 * torch.log(a1)\n",
        "    loss1 = -torch.sum(loss1)\n",
        "    loss2 = S(U_p1) * LS(U_p1)\n",
        "    loss2 = -torch.sum(loss2)\n",
        "    loss3 = S(U_p2) * LS(U_p2)\n",
        "    loss3 = -torch.sum(loss3)\n",
        "\n",
        "    return (loss1 - 0.5 * (loss2 + loss3))/U_batch_size\n",
        "\n",
        "\n",
        "def loss_diff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2):\n",
        "    S = nn.Softmax(dim = 1)\n",
        "    LS = nn.LogSoftmax(dim = 1)\n",
        "    \n",
        "    a = S(logit_S2) * LS(perturbed_logit_S1)\n",
        "    a = torch.sum(a)\n",
        "\n",
        "    b = S(logit_S1) * LS(perturbed_logit_S2)\n",
        "    b = torch.sum(b)\n",
        "\n",
        "    c = S(logit_U2) * LS(perturbed_logit_U1)\n",
        "    c = torch.sum(c)\n",
        "\n",
        "    d = S(logit_U1) * LS(perturbed_logit_U2)\n",
        "    d = torch.sum(d)\n",
        "\n",
        "    return -(a+b+c+d)/batch_size\n",
        "\n",
        "S_idx = []\n",
        "U_idx = []\n",
        "dataiter = iter(trainloader)\n",
        "train = [[] for x in range(args.num_class)]\n",
        "\n",
        "\n",
        "# Model\n",
        "if args.resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir(args.checkpoint_dir), 'Error: no checkpoint directory found!'\n",
        "    \n",
        "    checkpoint = torch.load('./'+ args.checkpoint_dir + '/ckpt.best.' + args.sess + '_' + str(args.seed))\n",
        "    \n",
        "    net1 = checkpoint['net1']\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    torch.set_rng_state(checkpoint['rng_state'])\n",
        "    torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])\n",
        "    np.random.set_state(checkpoint['np_state'])\n",
        "    random.setstate(checkpoint['random_state'])\n",
        "    \n",
        "    print('Building model..')\n",
        "    start_epoch = 0\n",
        "    net1 = co_train_classifier()\n",
        "    net2 = co_train_classifier()\n",
        "\n",
        "S_sampler = torch.utils.data.SubsetRandomSampler(S_idx)\n",
        "U_sampler = torch.utils.data.SubsetRandomSampler(U_idx)\n",
        "\n",
        "S_loader1 = torch.utils.data.DataLoader(trainset, batch_size=S_batch_size, sampler=S_sampler)\n",
        "U_loader = torch.utils.data.DataLoader(trainset, batch_size=U_batch_size, sampler=U_sampler)\n",
        "adversary1 = GradientSignAttack(\n",
        "net1, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"), eps=args.epsilon, clip_min=-math.inf, clip_max=math.inf, targeted=False)\n",
        "\n",
        "\n",
        "if args.dataset == 'cifar10':\n",
        "    step = int(len(trainset)/batch_size)\n",
        "else:\n",
        "    step = 1000\n",
        "\n",
        "    \n",
        "  \n",
        "net1.cuda()\n",
        "net2.cuda()\n",
        "net1 = torch.nn.DataParallel(net1)\n",
        "net2 = torch.nn.DataParallel(net2)\n",
        "print('Using', torch.cuda.device_count(), 'GPUs.')\n",
        "\n",
        "\n",
        "params = list(net1.parameters()) + list(net2.parameters())\n",
        "optimizer = optim.SGD(params, lr=args.base_lr, momentum=args.momentum, weight_decay=args.decay)\n",
        "\n",
        "def checkpoint(epoch, option):\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net1': net1,\n",
        "        'net2': net2,\n",
        "        'epoch': epoch,\n",
        "        'rng_state': torch.get_rng_state(),\n",
        "        'cuda_rng_state':torch.cuda.get_rng_state(),\n",
        "        'np_state': np.random.get_state(), \n",
        "        'random_state': random.getstate()\n",
        "    }\n",
        "    if not os.path.isdir(args.checkpoint_dir):\n",
        "        os.mkdir(args.checkpoint_dir)\n",
        "    if(option=='best'):\n",
        "        torch.save(state, './'+ args.checkpoint_dir +'/ckpt.best.' +\n",
        "                   args.sess + '_' + str(args.seed))\n",
        "    else:\n",
        "        torch.save(state, './'+ args.checkpoint_dir +'/ckpt.last.' +\n",
        "                   args.sess + '_' + str(args.seed))\n",
        "\n",
        "def train(epoch):\n",
        "    net1.train()\n",
        "    net2.train()\n",
        "\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    adjust_lamda(epoch)\n",
        "    \n",
        "    total_S1 = 0\n",
        "    total_S2 = 0\n",
        "    total_U1 = 0\n",
        "    total_U2 = 0\n",
        "    train_correct_S1 = 0\n",
        "    train_correct_S2 = 0\n",
        "    train_correct_U1 = 0\n",
        "    train_correct_U2 = 0\n",
        "    running_loss = 0.0\n",
        "    ls = 0.0\n",
        "    lc = 0.0 \n",
        "    ld = 0.0\n",
        "    S_iter1 = iter(S_loader1)\n",
        "    S_iter2 = iter(S_loader2)\n",
        "    U_iter = iter(U_loader)\n",
        "    print('epoch:', epoch+1)\n",
        "    for i in tqdm(range(step)):\n",
        "        inputs_S1, labels_S1 = S_iter1.next()\n",
        "        inputs_S2, labels_S2 = S_iter2.next()\n",
        "        inputs_U, labels_U = U_iter.next()\n",
        "\n",
        "        inputs_S1, labels_S1 = inputs_S1.cuda(), labels_S1.cuda()\n",
        "        inputs_S2, labels_S2 = inputs_S2.cuda(), labels_S2.cuda()\n",
        "        inputs_U = inputs_U.cuda()    \n",
        "\n",
        "\n",
        "        logit_S1 = net1(inputs_S1)\n",
        "        logit_S2 = net2(inputs_S2)\n",
        "        logit_U1 = net1(inputs_U)\n",
        "        logit_U2 = net2(inputs_U)\n",
        "\n",
        "        _, predictions_S1 = torch.max(logit_S1, 1)\n",
        "        _, predictions_S2 = torch.max(logit_S2, 1)\n",
        "        _, predictions_U1 = torch.max(logit_U1, 1)\n",
        "        _, predictions_U2 = torch.max(logit_U2, 1)\n",
        "        net1.eval()\n",
        "        net2.eval()\n",
        "        perturbed_data_S1 = adversary1.perturb(inputs_S1, labels_S1)\n",
        "        perturbed_data_U1 = adversary1.perturb(inputs_U, predictions_U1)\n",
        "\n",
        "        perturbed_data_S2 = adversary2.perturb(inputs_S2, labels_S2)\n",
        "        perturbed_data_U2 = adversary2.perturb(inputs_U, predictions_U2)\n",
        "        net1.train()\n",
        "        net2.train()\n",
        "\n",
        "        perturbed_logit_S1 = net1(perturbed_data_S2)\n",
        "        perturbed_logit_S2 = net2(perturbed_data_S1)\n",
        "\n",
        "        perturbed_logit_U1 = net1(perturbed_data_U2)\n",
        "        perturbed_logit_U2 = net2(perturbed_data_U1)\n",
        "        optimizer.zero_grad()\n",
        "        net1.zero_grad()\n",
        "        net2.zero_grad()\n",
        "\n",
        "        \n",
        "        Loss_sup = loss_sup(logit_S1, logit_S2, labels_S1, labels_S2)\n",
        "        Loss_cot = loss_cot(logit_U1, logit_U2)\n",
        "        Loss_diff = loss_diff(logit_S1, logit_S2, perturbed_logit_S1, perturbed_logit_S2, logit_U1, logit_U2, perturbed_logit_U1, perturbed_logit_U2)\n",
        "        \n",
        "        total_loss = Loss_sup + lambda_cot*Loss_cot + lambda_diff*Loss_diff\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_correct_S1 += np.sum(predictions_S1.cpu().numpy() == labels_S1.cpu().numpy())\n",
        "        total_S1 += labels_S1.size(0)\n",
        "\n",
        "        train_correct_U1 += np.sum(predictions_U1.cpu().numpy() == labels_U.cpu().numpy())\n",
        "        total_U1 += labels_U.size(0)\n",
        "\n",
        "        train_correct_S2 += np.sum(predictions_S2.cpu().numpy() == labels_S2.cpu().numpy())\n",
        "        total_S2 += labels_S2.size(0)\n",
        "\n",
        "        train_correct_U2 += np.sum(predictions_U2.cpu().numpy() == labels_U.cpu().numpy())\n",
        "        total_U2 += labels_U.size(0)\n",
        "        \n",
        "        running_loss += total_loss.item()\n",
        "        ls += Loss_sup.item()\n",
        "        lc += Loss_cot.item()\n",
        "        ld += Loss_diff.item()\n",
        "        writer.add_scalars('data/loss', {'loss_sup': Loss_sup.item(), 'loss_cot': Loss_cot.item(), 'loss_diff': Loss_diff.item()}, (epoch)*(step)+i)\n",
        "        writer.add_scalars('data/training_accuracy', {'net1 acc': 100. * (train_correct_S1+train_correct_U1) / (total_S1+total_U1), 'net2 acc': 100. * (train_correct_S2+train_correct_U2) / (total_S2+total_U2)}, (epoch)*(step)+i)\n",
        "        if (i+1)%50 == 0:\n",
        "            # print statistics\n",
        "            tqdm.write('net1 training acc: %.3f%% | total loss: %.3f | loss_sup: %.3f | loss_cot: %.3f | loss_diff: %.3f  '\n",
        "                % (100. * (train_correct_S1+train_correct_U1) / (total_S1+total_U1), 100. * (train_correct_S2+train_correct_U2) / (total_S2+total_U2), running_loss/(i+1), ls/(i+1), lc/(i+1), ld/(i+1)))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net1.eval()\n",
        "    net2.eval()\n",
        "    correct1 = 0\n",
        "    correct2 = 0\n",
        "    total1 = 0\n",
        "    total2 = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "            outputs1 = net1(inputs)\n",
        "            predicted1 = outputs1.max(1)\n",
        "            total1 += targets.size(0)\n",
        "            correct1 += predicted1[1].eq(targets).sum().item()\n",
        "\n",
        "            outputs2 = net2(inputs)\n",
        "            predicted2 = outputs2.max(1)\n",
        "            total2 += targets.size(0)\n",
        "            correct2 += predicted2[1].eq(targets).sum().item()\n",
        "\n",
        "    print('test acc: %.3f%% (%d/%d)'\n",
        "        % (100.*correct1/total1, correct1, total1, 100.*correct2/total2, correct2, total2))\n",
        "    writer.add_scalars('data/testing_accuracy', {'net1 acc': 100.*correct1/total1}, epoch)\n",
        "\n",
        "    acc = ((100.*correct1/total1)+(100.*correct2/total2))/2\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        checkpoint(epoch, 'best')\n",
        "\n",
        "for epoch in range(start_epoch, end_epoch):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    checkpoint(epoch, 'last')\n",
        "\n",
        "writer.export_scalars_to_json('./'+ args.tensorboard_dir + 'output.json')\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "w63XH3A2s0d-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}