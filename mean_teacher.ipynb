{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqlFdS9B7vpHfwjHBoAKBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thotakuria/surgical-tool-segmentation-using-deep-learning-techniques/blob/main/mean_teacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = transforms.Compose([\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor()])\n",
        "image_datasets = datasets.ImageFolder(root= \"/content/gdrive/MyDrive/dataset\", transform=data_transforms)\n",
        "dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "SQ96Ddu9Bnic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "def parameters_string(module):\n",
        "    lines = [ ]\n",
        "\n",
        "    row_format = \"{name:<40} {shape:>20} ={total_size:>12,d}\"\n",
        "    params = list(module.named_parameters())\n",
        "    for name, param in params:\n",
        "        lines.append(row_format.format(\n",
        "            name=name,\n",
        "            shape=\" * \".join(str(p) for p in param.size()),\n",
        "            total_size=param.numel()\n",
        "        ))\n",
        "    lines.append(\"=\" * 75)\n",
        "    lines.append(row_format.format(\n",
        "        name=\"all parameters\",\n",
        "        shape=\"sum of above\",\n",
        "        total_size=sum(int(param.numel()) for name, param in params)\n",
        "    ))\n",
        "    lines.append(\"\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def assert_exactly_one(lst):\n",
        "    assert sum(int(bool(el)) for el in lst) == 1, \", \".join(str(el)\n",
        "                                                            for el in lst)\n",
        "\n",
        "\n",
        "class AverageMeterSet:\n",
        "    def __init__(self):\n",
        "        self.meters = {}\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.meters[key]\n",
        "\n",
        "    def update(self, name, value, n=1):\n",
        "        if not name in self.meters:\n",
        "            self.meters[name] = AverageMeter()\n",
        "        self.meters[name].update(value, n)\n",
        "\n",
        "    def reset(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.reset()\n",
        "\n",
        "    def values(self, postfix=''):\n",
        "        return {name + postfix: meter.val for name, meter in self.meters.items()}\n",
        "\n",
        "    def averages(self, postfix='/avg'):\n",
        "        return {name + postfix: meter.avg for name, meter in self.meters.items()}\n",
        "\n",
        "    def sums(self, postfix='/sum'):\n",
        "        return {name + postfix: meter.sum for name, meter in self.meters.items()}\n",
        "\n",
        "    def counts(self, postfix='/count'):\n",
        "        return {name + postfix: meter.count for name, meter in self.meters.items()}\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __format__(self, format):\n",
        "        return \"{self.val:{format}} ({self.avg:{format}})\".format(self=self, format=format)\n",
        "\n",
        "\n",
        "def export(fn):\n",
        "    mod = sys.modules[fn.__module__]\n",
        "    if hasattr(mod, '__all__'):\n",
        "        mod.__all__.append(fn.__name__)\n",
        "    else:\n",
        "        mod.__all__ = [fn.__name__]\n",
        "    return fn\n",
        "\n",
        "\n",
        "def parameter_count(module):\n",
        "    return sum(int(param.numel()) for param in module.parameters())"
      ],
      "metadata": {
        "id": "ptfhJo7BWrYh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def softmax_mse_loss(input_logits, target_logits):\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_softmax = F.softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "    num_classes = input_logits.size()[1]\n",
        "    return F.mse_loss(input_softmax, target_softmax, size_average=False) / num_classes\n",
        "\n",
        "\n",
        "def softmax_kl_loss(input_logits, target_logits):\n",
        "    assert input_logits.size() == target_logits.size()\n",
        "    input_log_softmax = F.log_softmax(input_logits, dim=1)\n",
        "    target_softmax = F.softmax(target_logits, dim=1)\n",
        "    return F.kl_div(input_log_softmax, target_softmax, size_average=False)\n",
        "\n",
        "\n",
        "def symmetric_mse_loss(input1, input2):\n",
        "    assert input1.size() == input2.size()\n",
        "    num_classes = input1.size()[1]\n",
        "    return torch.sum((input1 - input2)**2) / num_classes"
      ],
      "metadata": {
        "id": "RXF-N94LW10H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import threading\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from pandas import DataFrame\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class TrainLog:\n",
        "    INCREMENTAL_UPDATE_TIME = 300\n",
        "\n",
        "    def __init__(self, directory, name):\n",
        "        self.log_file_path = \"{}/{}.msgpack\".format(directory, name)\n",
        "        self._log = defaultdict(dict)\n",
        "        self._log_lock = threading.RLock()\n",
        "        self._last_update_time = time.time() - self.INCREMENTAL_UPDATE_TIME\n",
        "\n",
        "    def record_single(self, step, column, value):\n",
        "        self._record(step, {column: value})\n",
        "\n",
        "    def record(self, step, col_val_dict):\n",
        "        self._record(step, col_val_dict)\n",
        "\n",
        "    def save(self):\n",
        "        df = self._as_dataframe()\n",
        "        df.to_msgpack(self.log_file_path, compress='zlib')\n",
        "\n",
        "    def _record(self, step, col_val_dict):\n",
        "        with self._log_lock:\n",
        "            self._log[step].update(col_val_dict)\n",
        "            if time.time() - self._last_update_time >= self.INCREMENTAL_UPDATE_TIME:\n",
        "                self._last_update_time = time.time()\n",
        "                self.save()\n",
        "\n",
        "    def _as_dataframe(self):\n",
        "        with self._log_lock:\n",
        "            return DataFrame.from_dict(self._log, orient='index')\n",
        "\n",
        "\n",
        "class RunContext:\n",
        "    def __init__(self, runner_file, run_idx):\n",
        "        logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "        runner_name = os.path.basename(runner_file).split(\".\")[0]\n",
        "        self.result_dir = \"{root}/{runner_name}/{date:%Y-%m-%d_%H:%M:%S}/{run_idx}\".format(\n",
        "            root='results',\n",
        "            runner_name=runner_name,\n",
        "            date=datetime.now(),\n",
        "            run_idx=run_idx\n",
        "        )\n",
        "        self.transient_dir = self.result_dir + \"/transient\"\n",
        "        os.makedirs(self.result_dir)\n",
        "        os.makedirs(self.transient_dir)\n",
        "\n",
        "    def create_train_log(self, name):\n",
        "        return TrainLog(self.result_dir, name)"
      ],
      "metadata": {
        "id": "CVDgNzVcW97y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import logging\n",
        "import os.path\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "\n",
        "LOG = logging.getLogger('main')\n",
        "NO_LABEL = -1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RandomTranslateWithReflect:\n",
        "    def __init__(self, max_translation):\n",
        "        self.max_translation = max_translation\n",
        "\n",
        "    def __call__(self, old_image):\n",
        "        xtranslation, ytranslation = np.random.randint(-self.max_translation,\n",
        "                                                       self.max_translation + 1,\n",
        "                                                       size=2)\n",
        "        xpad, ypad = abs(xtranslation), abs(ytranslation)\n",
        "        xsize, ysize = old_image.size\n",
        "\n",
        "        flipped_lr = old_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        flipped_tb = old_image.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "        flipped_both = old_image.transpose(Image.ROTATE_180)\n",
        "\n",
        "        new_image = Image.new(\"RGB\", (xsize + 2 * xpad, ysize + 2 * ypad))\n",
        "\n",
        "        new_image.paste(old_image, (xpad, ypad))\n",
        "\n",
        "        new_image.paste(flipped_lr, (xpad + xsize - 1, ypad))\n",
        "        new_image.paste(flipped_lr, (xpad - xsize + 1, ypad))\n",
        "\n",
        "        new_image.paste(flipped_tb, (xpad, ypad + ysize - 1))\n",
        "        new_image.paste(flipped_tb, (xpad, ypad - ysize + 1))\n",
        "\n",
        "        new_image.paste(flipped_both, (xpad - xsize + 1, ypad - ysize + 1))\n",
        "        new_image.paste(flipped_both, (xpad + xsize - 1, ypad - ysize + 1))\n",
        "        new_image.paste(flipped_both, (xpad - xsize + 1, ypad + ysize - 1))\n",
        "        new_image.paste(flipped_both, (xpad + xsize - 1, ypad + ysize - 1))\n",
        "\n",
        "        new_image = new_image.crop((xpad - xtranslation,\n",
        "                                    ypad - ytranslation,\n",
        "                                    xpad + xsize - xtranslation,\n",
        "                                    ypad + ysize - ytranslation))\n",
        "\n",
        "        return new_image\n",
        "\n",
        "\n",
        "class TransformTwice:\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, inp):\n",
        "        out1 = self.transform(inp)\n",
        "        out2 = self.transform(inp)\n",
        "        return out1, out2\n",
        "\n",
        "\n",
        "def relabel_dataset(dataset, labels):\n",
        "    unlabeled_idxs = []\n",
        "    for idx in range(len(dataset.imgs)):\n",
        "        path, _ = dataset.imgs[idx]\n",
        "        filename = os.path.basename(path)\n",
        "        if filename in labels:\n",
        "            label_idx = dataset.class_to_idx[labels[filename]]\n",
        "            dataset.imgs[idx] = path, label_idx\n",
        "            del labels[filename]\n",
        "        else:\n",
        "            dataset.imgs[idx] = path, NO_LABEL\n",
        "            unlabeled_idxs.append(idx)\n",
        "\n",
        "    if len(labels) != 0:\n",
        "        message = \"List of unlabeled contains {} unknown files: {}, ...\"\n",
        "        some_missing = ', '.join(list(labels.keys())[:5])\n",
        "        raise LookupError(message.format(len(labels), some_missing))\n",
        "\n",
        "    labeled_idxs = sorted(set(range(len(dataset.imgs))) - set(unlabeled_idxs))\n",
        "\n",
        "    return labeled_idxs, unlabeled_idxs\n",
        "\n",
        "\n",
        "class TwoStreamBatchSampler(Sampler):\n",
        "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
        "        self.primary_indices = primary_indices\n",
        "        self.secondary_indices = secondary_indices\n",
        "        self.secondary_batch_size = secondary_batch_size\n",
        "        self.primary_batch_size = batch_size - secondary_batch_size\n",
        "\n",
        "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
        "        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        primary_iter = iterate_once(self.primary_indices)\n",
        "        secondary_iter = iterate_eternally(self.secondary_indices)\n",
        "        return (\n",
        "            primary_batch + secondary_batch\n",
        "            for (primary_batch, secondary_batch)\n",
        "            in  zip(grouper(primary_iter, self.primary_batch_size),\n",
        "                    grouper(secondary_iter, self.secondary_batch_size))\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.primary_indices) // self.primary_batch_size\n",
        "\n",
        "\n",
        "def iterate_once(iterable):\n",
        "    return np.random.permutation(iterable)\n",
        "\n",
        "\n",
        "def iterate_eternally(indices):\n",
        "    def infinite_shuffles():\n",
        "        while True:\n",
        "            yield np.random.permutation(indices)\n",
        "    return itertools.chain.from_iterable(infinite_shuffles())\n",
        "\n",
        "\n",
        "def grouper(iterable, n):\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip(*args)"
      ],
      "metadata": {
        "id": "cUYbHgkiXjOD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable, Function\n",
        "\n",
        "def shakeshake26(pretrained=False, **kwargs):\n",
        "    assert not pretrained\n",
        "    model = ResNet32x32(ShakeShakeBlock,\n",
        "                        layers=[4, 4, 4],\n",
        "                        channels=96,\n",
        "                        downsample='shift_conv', **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnext152(pretrained=False, **kwargs):\n",
        "    assert not pretrained\n",
        "    model = ResNet224x224(BottleneckBlock,\n",
        "                          layers=[3, 8, 36, 3],\n",
        "                          channels=32 * 4,\n",
        "                          groups=32,\n",
        "                          downsample='basic', **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "class ResNet224x224(nn.Module):\n",
        "    def __init__(self, block, layers, channels, groups=1, num_classes=1000, downsample='basic'):\n",
        "        super().__init__()\n",
        "        assert len(layers) == 4\n",
        "        self.downsample_mode = downsample\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, channels, groups, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, channels * 2, groups, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, channels * 4, groups, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, channels * 8, groups, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.fc1 = nn.Linear(block.out_channels(\n",
        "            channels * 8, groups), num_classes)\n",
        "        self.fc2 = nn.Linear(block.out_channels(\n",
        "            channels * 8, groups), num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, groups, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != block.out_channels(planes, groups):\n",
        "            if self.downsample_mode == 'basic' or stride == 1:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv2d(self.inplanes, block.out_channels(planes, groups),\n",
        "                              kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(block.out_channels(planes, groups)),\n",
        "                )\n",
        "            elif self.downsample_mode == 'shift_conv':\n",
        "                downsample = ShiftConvDownsample(in_channels=self.inplanes,\n",
        "                                                 out_channels=block.out_channels(planes, groups))\n",
        "            else:\n",
        "                assert False\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, groups, stride, downsample))\n",
        "        self.inplanes = block.out_channels(planes, groups)\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x), self.fc2(x)\n",
        "\n",
        "\n",
        "class ResNet32x32(nn.Module):\n",
        "    def __init__(self, block, layers, channels, groups=1, num_classes=1000, downsample='basic'):\n",
        "        super().__init__()\n",
        "        assert len(layers) == 3\n",
        "        self.downsample_mode = downsample\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.layer1 = self._make_layer(block, channels, groups, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, channels * 2, groups, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, channels * 4, groups, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc1 = nn.Linear(block.out_channels(\n",
        "            channels * 4, groups), num_classes)\n",
        "        self.fc2 = nn.Linear(block.out_channels(\n",
        "            channels * 4, groups), num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, groups, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != block.out_channels(planes, groups):\n",
        "            if self.downsample_mode == 'basic' or stride == 1:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv2d(self.inplanes, block.out_channels(planes, groups),\n",
        "                              kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(block.out_channels(planes, groups)),\n",
        "                )\n",
        "            elif self.downsample_mode == 'shift_conv':\n",
        "                downsample = ShiftConvDownsample(in_channels=self.inplanes,\n",
        "                                                 out_channels=block.out_channels(planes, groups))\n",
        "            else:\n",
        "                assert False\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, groups, stride, downsample))\n",
        "        self.inplanes = block.out_channels(planes, groups)\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x), self.fc2(x)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    @classmethod\n",
        "    def out_channels(cls, planes, groups):\n",
        "        if groups > 1:\n",
        "            return 2 * planes\n",
        "        else:\n",
        "            return 4 * planes\n",
        "\n",
        "    def __init__(self, inplanes, planes, groups, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv_a1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn_a1 = nn.BatchNorm2d(planes)\n",
        "        self.conv_a2 = nn.Conv2d(\n",
        "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False, groups=groups)\n",
        "        self.bn_a2 = nn.BatchNorm2d(planes)\n",
        "        self.conv_a3 = nn.Conv2d(planes, self.out_channels(\n",
        "            planes, groups), kernel_size=1, bias=False)\n",
        "        self.bn_a3 = nn.BatchNorm2d(self.out_channels(planes, groups))\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        a, residual = x, x\n",
        "\n",
        "        a = self.conv_a1(a)\n",
        "        a = self.bn_a1(a)\n",
        "        a = self.relu(a)\n",
        "        a = self.conv_a2(a)\n",
        "        a = self.bn_a2(a)\n",
        "        a = self.relu(a)\n",
        "        a = self.conv_a3(a)\n",
        "        a = self.bn_a3(a)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(residual)\n",
        "\n",
        "        return self.relu(residual + a)\n",
        "\n",
        "\n",
        "class ShakeShakeBlock(nn.Module):\n",
        "    @classmethod\n",
        "    def out_channels(cls, planes, groups):\n",
        "        assert groups == 1\n",
        "        return planes\n",
        "\n",
        "    def __init__(self, inplanes, planes, groups, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        assert groups == 1\n",
        "        self.conv_a1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn_a1 = nn.BatchNorm2d(planes)\n",
        "        self.conv_a2 = conv3x3(planes, planes)\n",
        "        self.bn_a2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv_b1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn_b1 = nn.BatchNorm2d(planes)\n",
        "        self.conv_b2 = conv3x3(planes, planes)\n",
        "        self.bn_b2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        a, b, residual = x, x, x\n",
        "\n",
        "        a = F.relu(a, inplace=False)\n",
        "        a = self.conv_a1(a)\n",
        "        a = self.bn_a1(a)\n",
        "        a = F.relu(a, inplace=True)\n",
        "        a = self.conv_a2(a)\n",
        "        a = self.bn_a2(a)\n",
        "\n",
        "        b = F.relu(b, inplace=False)\n",
        "        b = self.conv_b1(b)\n",
        "        b = self.bn_b1(b)\n",
        "        b = F.relu(b, inplace=True)\n",
        "        b = self.conv_b2(b)\n",
        "        b = self.bn_b2(b)\n",
        "\n",
        "        ab = shake(a, b, training=self.training)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        return residual + ab\n",
        "\n",
        "\n",
        "class Shake(Function):\n",
        "    @classmethod\n",
        "    def forward(cls, ctx, inp1, inp2, training):\n",
        "        assert inp1.size() == inp2.size()\n",
        "        gate_size = [inp1.size()[0], *itertools.repeat(1, inp1.dim() - 1)]\n",
        "        gate = inp1.new(*gate_size)\n",
        "        if training:\n",
        "            gate.uniform_(0, 1)\n",
        "        else:\n",
        "            gate.fill_(0.5)\n",
        "        return inp1 * gate + inp2 * (1. - gate)\n",
        "\n",
        "    @classmethod\n",
        "    def backward(cls, ctx, grad_output):\n",
        "        grad_inp1 = grad_inp2 = grad_training = None\n",
        "        gate_size = [grad_output.size()[0], *itertools.repeat(1,\n",
        "                                                              grad_output.dim() - 1)]\n",
        "        gate = Variable(grad_output.data.new(*gate_size).uniform_(0, 1))\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_inp1 = grad_output * gate\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_inp2 = grad_output * (1 - gate)\n",
        "        assert not ctx.needs_input_grad[2]\n",
        "        return grad_inp1, grad_inp2, grad_training\n",
        "\n",
        "\n",
        "def shake(inp1, inp2, training=False):\n",
        "    return Shake.apply(inp1, inp2, training)\n",
        "\n",
        "\n",
        "class ShiftConvDownsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels=2 * in_channels,\n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=1,\n",
        "                              groups=2)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat((x[:, :, 0::2, 0::2],\n",
        "                       x[:, :, 1::2, 1::2]), dim=1)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kv5Leyb4Yo06"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLOYfYYAWAaP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import math\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "import torchvision.datasets\n",
        "LOG = logging.getLogger('main')\n",
        "\n",
        "args = None\n",
        "best_prec1 = 0\n",
        "global_step = 0\n",
        "\n",
        "\n",
        "def main(context):\n",
        "    global global_step\n",
        "    global best_prec1\n",
        "\n",
        "    checkpoint_path = context.transient_dir\n",
        "    training_log = context.create_train_log(\"training\")\n",
        "    validation_log = context.create_train_log(\"validation\")\n",
        "    ema_validation_log = context.create_train_log(\"ema_validation\")\n",
        "\n",
        "    dataset_config = datasets.__dict__[args.dataset]()\n",
        "    num_classes = dataset_config.pop('num_classes')\n",
        "    train_loader, eval_loader = create_data_loaders(**dataset_config, args=args)\n",
        "\n",
        "    def create_model(ema=False):\n",
        "        LOG.info(\"=> creating {pretrained}{ema}model '{arch}'\".format(\n",
        "            pretrained='pre-trained ' if args.pretrained else '',\n",
        "            ema='EMA ' if ema else '',\n",
        "            arch=args.arch))\n",
        "\n",
        "        model_factory = architectures.__dict__[args.arch]\n",
        "        model_params = dict(pretrained=args.pretrained, num_classes=num_classes)\n",
        "        model = model_factory(**model_params)\n",
        "        model = nn.DataParallel(model).cuda()\n",
        "\n",
        "        if ema:\n",
        "            for param in model.parameters():\n",
        "                param.detach_()\n",
        "\n",
        "        return model\n",
        "\n",
        "    model = create_model()\n",
        "    ema_model = create_model(ema=True)\n",
        "\n",
        "    LOG.info(parameters_string(model))\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay,\n",
        "                                nesterov=args.nesterov)\n",
        "    if args.resume:\n",
        "        assert os.path.isfile(args.resume), \"=> no checkpoint found at '{}'\".format(args.resume)\n",
        "        LOG.info(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "        checkpoint = torch.load(args.resume)\n",
        "        args.start_epoch = checkpoint['epoch']\n",
        "        global_step = checkpoint['global_step']\n",
        "        best_prec1 = checkpoint['best_prec1']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        ema_model.load_state_dict(checkpoint['ema_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        LOG.info(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    if args.evaluate:\n",
        "        LOG.info(\"Evaluating the primary model:\")\n",
        "        validate(eval_loader, model, validation_log, global_step, args.start_epoch)\n",
        "        LOG.info(\"Evaluating the EMA model:\")\n",
        "        validate(eval_loader, ema_model, ema_validation_log, global_step, args.start_epoch)\n",
        "        return\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        start_time = time.time()\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, ema_model, optimizer, epoch, training_log)\n",
        "        LOG.info(\"--- training epoch in %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "        if args.evaluation_epochs and (epoch + 1) % args.evaluation_epochs == 0:\n",
        "            start_time = time.time()\n",
        "            LOG.info(\"=\")\n",
        "            prec1 = validate(eval_loader, model, validation_log, global_step, epoch + 1)\n",
        "            LOG.info(\"=\")\n",
        "            ema_prec1 = validate(eval_loader, ema_model, ema_validation_log, global_step, epoch + 1)\n",
        "            LOG.info(\"--\" % (time.time() - start_time))\n",
        "            is_best = ema_prec1 > best_prec1\n",
        "            best_prec1 = max(ema_prec1, best_prec1)\n",
        "        else:\n",
        "            is_best = False\n",
        "\n",
        "        if args.checkpoint_epochs and (epoch + 1) % args.checkpoint_epochs == 0:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'global_step': global_step,\n",
        "                'arch': args.arch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'ema_state_dict': ema_model.state_dict(),\n",
        "                'best_prec1': best_prec1,\n",
        "                'optimizer' : optimizer.state_dict(),\n",
        "            }, is_best, checkpoint_path, epoch + 1)\n",
        "\n",
        "\n",
        "def parse_dict_args(**kwargs):\n",
        "    global args\n",
        "\n",
        "    def to_cmdline_kwarg(key, value):\n",
        "        if len(key) == 1:\n",
        "            key = \"-{}\".format(key)\n",
        "        else:\n",
        "            key = \"--{}\".format(re.sub(r\"_\", \"-\", key))\n",
        "        value = str(value)\n",
        "        return key, value\n",
        "\n",
        "    kwargs_pairs = (to_cmdline_kwarg(key, value)\n",
        "                    for key, value in kwargs.items())\n",
        "    cmdline_args = list(sum(kwargs_pairs, ()))\n",
        "    args = parser.parse_args(cmdline_args)\n",
        "\n",
        "\n",
        "def create_data_loaders(train_transformation,\n",
        "                        eval_transformation,\n",
        "                        datadir,\n",
        "                        args):\n",
        "    traindir = os.path.join(datadir, args.train_subdir)\n",
        "    evaldir = os.path.join(datadir, args.eval_subdir)\n",
        "\n",
        "    assert_exactly_one([args.exclude_unlabeled, args.labeled_batch_size])\n",
        "\n",
        "    dataset = torchvision.datasets.ImageFolder(traindir, train_transformation)\n",
        "\n",
        "    if args.labels:\n",
        "        with open(args.labels) as f:\n",
        "            labels = dict(line.split(' ') for line in f.read().splitlines())\n",
        "        labeled_idxs, unlabeled_idxs = data.relabel_dataset(dataset, labels)\n",
        "\n",
        "    if args.exclude_unlabeled:\n",
        "        sampler = SubsetRandomSampler(labeled_idxs)\n",
        "        batch_sampler = BatchSampler(sampler, args.batch_size, drop_last=True)\n",
        "    elif args.labeled_batch_size:\n",
        "        batch_sampler = data.TwoStreamBatchSampler(\n",
        "            unlabeled_idxs, labeled_idxs, args.batch_size, args.labeled_batch_size)\n",
        "    else:\n",
        "        assert False, \"labeled batch size {}\".format(args.labeled_batch_size)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                               batch_sampler=batch_sampler,\n",
        "                                               num_workers=args.workers,\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    eval_loader = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.ImageFolder(evaldir, eval_transformation),\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2 * args.workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=False)\n",
        "\n",
        "    return train_loader, eval_loader\n",
        "\n",
        "\n",
        "def update_ema_variables(model, ema_model, alpha, global_step):\n",
        "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
        "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
        "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
        "\n",
        "\n",
        "def train(train_loader, model, ema_model, optimizer, epoch, log):\n",
        "    global global_step\n",
        "\n",
        "    class_criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL).cuda()\n",
        "    if args.consistency_type == 'mse':\n",
        "        consistency_criterion = losses.softmax_mse_loss\n",
        "    elif args.consistency_type == 'kl':\n",
        "        consistency_criterion = losses.softmax_kl_loss\n",
        "    else:\n",
        "        assert False, args.consistency_type\n",
        "    residual_logit_criterion = losses.symmetric_mse_loss\n",
        "\n",
        "    meters = AverageMeterSet()\n",
        "    model.train()\n",
        "    ema_model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, ((input, ema_input), target) in enumerate(train_loader):\n",
        "        meters.update('data_time', time.time() - end)\n",
        "\n",
        "        adjust_learning_rate(optimizer, epoch, i, len(train_loader))\n",
        "        meters.update('lr', optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        ema_input_var = torch.autograd.Variable(ema_input, volatile=True)\n",
        "        target_var = torch.autograd.Variable(target.cuda(async= True))\n",
        "\n",
        "        minibatch_size = len(target_var)\n",
        "        labeled_minibatch_size = target_var.data.ne(NO_LABEL).sum()\n",
        "        assert labeled_minibatch_size > 0\n",
        "        meters.update('labeled_minibatch_size', labeled_minibatch_size)\n",
        "\n",
        "        ema_model_out = ema_model(ema_input_var)\n",
        "        model_out = model(input_var)\n",
        "\n",
        "        if isinstance(model_out, Variable):\n",
        "            assert args.logit_distance_cost < 0\n",
        "            logit1 = model_out\n",
        "            ema_logit = ema_model_out\n",
        "        else:\n",
        "            assert len(model_out) == 2\n",
        "            assert len(ema_model_out) == 2\n",
        "            logit1, logit2 = model_out\n",
        "            ema_logit, _ = ema_model_out\n",
        "\n",
        "        ema_logit = Variable(ema_logit.detach().data, requires_grad=False)\n",
        "\n",
        "        if args.logit_distance_cost >= 0:\n",
        "            class_logit, cons_logit = logit1, logit2\n",
        "            res_loss = args.logit_distance_cost * residual_logit_criterion(class_logit, cons_logit) / minibatch_size\n",
        "            meters.update('res_loss', res_loss.data[0])\n",
        "        else:\n",
        "            class_logit, cons_logit = logit1, logit1\n",
        "            res_loss = 0\n",
        "\n",
        "        class_loss = class_criterion(class_logit, target_var) / minibatch_size\n",
        "        meters.update('class_loss', class_loss.data[0])\n",
        "\n",
        "        ema_class_loss = class_criterion(ema_logit, target_var) / minibatch_size\n",
        "        meters.update('ema_class_loss', ema_class_loss.data[0])\n",
        "\n",
        "        if args.consistency:\n",
        "            consistency_weight = get_current_consistency_weight(epoch)\n",
        "            meters.update('cons_weight', consistency_weight)\n",
        "            consistency_loss = consistency_weight * consistency_criterion(cons_logit, ema_logit) / minibatch_size\n",
        "            meters.update('cons_loss', consistency_loss.data[0])\n",
        "        else:\n",
        "            consistency_loss = 0\n",
        "            meters.update('cons_loss', 0)\n",
        "\n",
        "        loss = class_loss + consistency_loss + res_loss\n",
        "        assert not (np.isnan(loss.data[0]) or loss.data[0] > 1e5), 'Loss explosion: {}'.format(loss.data[0])\n",
        "        meters.update('loss', loss.data[0])\n",
        "\n",
        "        prec1, prec5 = accuracy(class_logit.data, target_var.data, topk=(1, 5))\n",
        "        meters.update('top1', prec1[0], labeled_minibatch_size)\n",
        "        meters.update('error1', 100. - prec1[0], labeled_minibatch_size)\n",
        "        meters.update('top5', prec5[0], labeled_minibatch_size)\n",
        "        meters.update('error5', 100. - prec5[0], labeled_minibatch_size)\n",
        "\n",
        "        ema_prec1, ema_prec5 = accuracy(ema_logit.data, target_var.data, topk=(1, 5))\n",
        "        meters.update('ema_top1', ema_prec1[0], labeled_minibatch_size)\n",
        "        meters.update('ema_error1', 100. - ema_prec1[0], labeled_minibatch_size)\n",
        "        meters.update('ema_top5', ema_prec5[0], labeled_minibatch_size)\n",
        "        meters.update('ema_error5', 100. - ema_prec5[0], labeled_minibatch_size)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        global_step += 1\n",
        "        update_ema_variables(model, ema_model, args.ema_decay, global_step)\n",
        "        meters.update('batch_time', time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            LOG.info(\n",
        "                'Epoch: [{0}][{1}/{2}]\\t'\n",
        "                'Time {meters[batch_time]:.3f}\\t'\n",
        "                'Data {meters[data_time]:.3f}\\t'\n",
        "                'Cons {meters[cons_loss]:.4f}\\t'\n",
        "                'Prec@1 {meters[top1]:.3f}\\t'\n",
        "                'Prec@5 {meters[top5]:.3f}'.format(\n",
        "                    epoch, i, len(train_loader), meters=meters))\n",
        "            log.record(epoch + i / len(train_loader), {\n",
        "                'step': global_step,\n",
        "                **meters.values(),\n",
        "                **meters.averages(),\n",
        "                **meters.sums()\n",
        "            })\n",
        "\n",
        "\n",
        "def validate(eval_loader, model, log, global_step, epoch):\n",
        "    class_criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL).cuda()\n",
        "    meters = AverageMeterSet()\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(eval_loader):\n",
        "        meters.update('data_time', time.time() - end)\n",
        "\n",
        "        input_var = torch.autograd.Variable(input, volatile=True)\n",
        "        target_var = torch.autograd.Variable(target.cuda(async=True), volatile=True)\n",
        "\n",
        "        minibatch_size = len(target_var)\n",
        "        labeled_minibatch_size = target_var.data.ne(NO_LABEL).sum()\n",
        "        assert labeled_minibatch_size > 0\n",
        "        meters.update('labeled_minibatch_size', labeled_minibatch_size)\n",
        "        output1, output2 = model(input_var)\n",
        "        softmax1, softmax2 = F.softmax(output1, dim=1), F.softmax(output2, dim=1)\n",
        "        class_loss = class_criterion(output1, target_var) / minibatch_size\n",
        "        prec1, prec5 = accuracy(output1.data, target_var.data, topk=(1, 5))\n",
        "        meters.update('class_loss', class_loss.data[0], labeled_minibatch_size)\n",
        "        meters.update('top1', prec1[0], labeled_minibatch_size)\n",
        "        meters.update('error1', 100.0 - prec1[0], labeled_minibatch_size)\n",
        "        meters.update('top5', prec5[0], labeled_minibatch_size)\n",
        "        meters.update('error5', 100.0 - prec5[0], labeled_minibatch_size)\n",
        "        meters.update('batch_time', time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            LOG.info(\n",
        "                'Test: [{0}/{1}]\\t'\n",
        "                'Time {meters[batch_time]:.3f}\\t'\n",
        "                'Data {meters[data_time]:.3f}\\t'               \n",
        "                'Prec@1 {meters[top1]:.3f}\\t'\n",
        "                'Prec@5 {meters[top5]:.3f}'.format(\n",
        "                    i, len(eval_loader), meters=meters))\n",
        "\n",
        "    LOG.info(' * Prec@1 {top1.avg:.3f}\\tPrec@5 {top5.avg:.3f}'.format(top1=meters['top1'], top5=meters['top5']))\n",
        "    log.record(epoch, {\n",
        "        'step': global_step,\n",
        "        **meters.values(),\n",
        "        **meters.averages(),\n",
        "        **meters.sums()\n",
        "    })\n",
        "\n",
        "    return meters['top1'].avg\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, dirpath, epoch):\n",
        "    filename = 'checkpoint.{}.ckpt'.format(epoch)\n",
        "    checkpoint_path = os.path.join(dirpath, filename)\n",
        "    best_path = os.path.join(dirpath, 'best.ckpt')\n",
        "    torch.save(state, checkpoint_path)\n",
        "    LOG.info(\"-\" % checkpoint_path)\n",
        "    if is_best:\n",
        "        shutil.copyfile(checkpoint_path, best_path)\n",
        "        LOG.info(\"-\" % best_path)\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, step_in_epoch, total_steps_in_epoch):\n",
        "    lr = args.lr\n",
        "    epoch = epoch + step_in_epoch / total_steps_in_epoch\n",
        "    lr = ramps.linear_rampup(epoch, args.lr_rampup) * (args.lr - args.initial_lr) + args.initial_lr\n",
        "    if args.lr_rampdown_epochs:\n",
        "        assert args.lr_rampdown_epochs >= args.epochs\n",
        "        lr *= ramps.cosine_rampdown(epoch, args.lr_rampdown_epochs)\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def get_current_consistency_weight(epoch):\n",
        "    return args.consistency * ramps.sigmoid_rampup(epoch, args.consistency_rampup)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "maxk = max(topk)\n",
        "    labeled_minibatch_size = max(target.ne(NO_LABEL).sum(), 1e-8)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / labeled_minibatch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    args = cli.parse_commandline_args()\n",
        "    main(RunContext(__file__, 0))"
      ]
    }
  ]
}