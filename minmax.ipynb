{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6ub/DHHNbe1/zCe3HzaNa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thotakuria/surgical-tool-segmentation-using-deep-learning-techniques/blob/main/minmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = transforms.Compose([\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor()])\n",
        "image_datasets = datasets.ImageFolder(root= \"/content/gdrive/MyDrive/dataset\", transform=data_transforms)\n",
        "dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "Fj8t7Qd-tlvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "\n",
        "def create_exp_dir(path, desc='Experiment dir: {}'):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    print(desc.format(path))\n",
        "\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def get_logger(log_dir):\n",
        "    create_exp_dir(log_dir)\n",
        "    log_format = '%(asctime)s %(message)s'\n",
        "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "    fh = logging.FileHandler(os.path.join(log_dir, 'run.log'))\n",
        "    fh.setFormatter(logging.Formatter(log_format))\n",
        "    logger = logging.getLogger('Nas Seg')\n",
        "    logger.addHandler(fh)\n",
        "    return logger\n"
      ],
      "metadata": {
        "id": "gTd0qs19YuSx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CONV_Block(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
        "        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_1 = self.conv(x)\n",
        "        y_1 = self.bn(y_1)\n",
        "        y_1 = self.relu(y_1)\n",
        "        \n",
        "\n",
        "        return y_1\n",
        "\n",
        "\n",
        "class projectors(nn.Module):\n",
        "    def __init__(self, input_nc=1, ndf=8, norm_layer=nn.BatchNorm2d):\n",
        "        super(projectors, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv_1 = conv(input_nc, ndf)\n",
        "        self.conv_2 = conv(ndf, ndf*2)\n",
        "        self.final = nn.Conv2d(ndf*2, ndf*2, kernel_size=1)\n",
        "    def forward(self, input):\n",
        "        x_0 = self.conv_1(input)\n",
        "        x_0 = self.pool(x_0)\n",
        "        x_out = self.conv_2(x_0)\n",
        "        x_out = self.pool(x_out)\n",
        "        x_out = self.final(x_out)\n",
        "        return x_out    \n",
        "    \n",
        "class classifier(nn.Module):\n",
        "    def __init__(self, inp_dim = 1,ndf=8, norm_layer=nn.BatchNorm2d):\n",
        "        super(classifier, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv_1 = conv(inp_dim, ndf)\n",
        "        self.conv_2 = conv(ndf, ndf*2)\n",
        "        self.conv_3 = conv(ndf*2, ndf*4)\n",
        "        self.final = nn.Conv2d(ndf*4, ndf*4, kernel_size=1)\n",
        "    def forward(self,input):\n",
        "        x_0 = self.conv_1(input)\n",
        "        x_0 = self.pool(x_0)\n",
        "        x_1 = self.conv_2(x_0)\n",
        "        x_1 = self.pool(x_1)\n",
        "        x_2 = self.conv_3(x_1)\n",
        "        x_2 = self.pool(x_2)\n",
        "        # x_out = self.linear(x_2)\n",
        "        x_out = self.final(x_2)\n",
        "        return x_out\n",
        "   \n",
        "      \n",
        "            "
      ],
      "metadata": {
        "id": "hry-WkSMhmAn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torchvision.models as models\n",
        "import os\n",
        "\n",
        "class CONV_Block(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
        "        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class preUnet(nn.Module):\n",
        "    def __init__(self, num_classes=1, input_channels=3, **kwargs):\n",
        "        super().__init__()\n",
        "        self.resnet = res2net101_v1b_26w_4s(pretrained=True)\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "\n",
        "        self.conv_up_1 = CONV_Block(1024, 1024, 512)\n",
        "        self.conv_up_2 = CONV_Block(1024, 512, 512)\n",
        "        self.conv_up_3 = CONV_Block(512, 512, 256)\n",
        "        self.conv_up_4 = CONV_Block(512, 256, 256)\n",
        "        self.conv_up_5 = CONV_Block(256, 256, 64)\n",
        "        self.conv_up_6 = CONV_Block(128, 64, 64)\n",
        "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x_k = self.resnet.maxpool(x)\n",
        "        x1 = self.resnet.layer1(x_k)\n",
        "        x2 = self.resnet.layer2(x1)\n",
        "        \n",
        "        x3 = self.resnet.layer3(x2)\n",
        "        \n",
        "        x_up_1 = self.conv_up_1(self.up(x3))\n",
        "        x_up_1 = self.conv_up_2(torch.cat([x2, x_up_1], 1))\n",
        "        \n",
        "        x_up_2 = self.conv_up_3(self.up(x_up_1))\n",
        "        x_up_2 = self.conv_up_4(torch.cat([x1, x_up_2], 1))\n",
        "\n",
        "        x_up_3 = self.conv_up_5(self.up(x_up_2))\n",
        "        x_up_3 = self.conv_up_6(torch.cat([x, x_up_3], 1))\n",
        "        \n",
        "        x_up_4 = self.up(x_up_3)\n",
        "        output = self.final(x_up_4)\n",
        "        return output"
      ],
      "metadata": {
        "id": "NxnNwTtvhe2O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "from datetime import datetime\n",
        "from distutils.dir_util import copy_tree\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--epoch', type=int, default=100, help='epoch number')\n",
        "parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
        "parser.add_argument('--batchsize', type=int, default=1, help='training batch size')\n",
        "parser.add_argument('--trainsize', type=int, default=(512,288), help='training dataset size')\n",
        "parser.add_argument('--dataset', type=str, default='kvasir', help='dataset name')\n",
        "parser.add_argument('--split', type=float, default=1, help='training data ratio')\n",
        "parser.add_argument('--momentum', default=0.9, type=float)\n",
        "parser.add_argument('--ratio', type=float, default=0.5, help='labeled data ratio')\n",
        "opt = parser.parse_args()\n",
        "pixel_wise_contrastive_loss_criter = ConLoss()\n",
        "contrastive_loss_sup_criter = contrastive_loss_sup()\n",
        "\n",
        "\n",
        "def adjust_lr(optimizer, init_lr, epoch, max_epoch):\n",
        "    lr_ = init_lr * (1.0 - epoch / max_epoch) ** 0.9\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr_\n",
        "\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self):\n",
        "        self.patience = 0\n",
        "        self.best_dice_coeff_1 = False\n",
        "        self.best_dice_coeff_2 = False\n",
        "        self.model_1 = preUnet()\n",
        "        self.model_2 = preUnet()\n",
        "        self.projector_1 = projectors()\n",
        "        self.projector_2 = projectors()\n",
        "        self.classifier_1 = classifier()\n",
        "        self.classifier_2 = classifier()\n",
        "        self.best_mIoU, self.best_dice_coeff = 0, 0\n",
        "        self._init_configure()\n",
        "        self._init_logger()\n",
        "\n",
        "    def _init_configure(self):\n",
        "        with open('configs/config.yml') as fp:\n",
        "            self.cfg = yaml.safe_load(fp)\n",
        "\n",
        "    def _init_logger(self):\n",
        "\n",
        "        log_dir = 'logs/' + opt.dataset + '/train/'\n",
        "\n",
        "        self.logger = get_logger(log_dir)\n",
        "        print('RUNDIR: {}'.format(log_dir))\n",
        "\n",
        "        self.save_path = log_dir\n",
        "        self.image_save_path_1 = log_dir + \"/saved_images_1\"\n",
        "        self.image_save_path_2 = log_dir + \"/saved_images_2\"\n",
        "\n",
        "        create_dir(self.image_save_path_1)\n",
        "        create_dir(self.image_save_path_2)\n",
        "\n",
        "        self.save_tbx_log = self.save_path + '/tbx_log'\n",
        "        self.writer = SummaryWriter(self.save_tbx_log)\n",
        "\n",
        "    def run(self):\n",
        "        print('Learning Rate:'.format(opt.lr))\n",
        "        self.model_1.cuda()\n",
        "        self.model_2.cuda()\n",
        "        \n",
        "        params = list(self.model_1.parameters()) + list(self.model_2.parameters())\n",
        "\n",
        "        optimizer = torch.optim.Adam(params,lr=opt.lr)\n",
        "\n",
        "\n",
        "        image_root = './data/'+ opt.dataset +'/train/image/'\n",
        "        gt_root = './data/'+ opt.dataset +'/train/mask/'\n",
        "        val_img_root = './data/'+ opt.dataset +'/test/image/'\n",
        "        val_gt_root = './data/'+ opt.dataset +'/test/mask/'\n",
        "\n",
        "\n",
        "        self.logger.info(\"Split Percentage : {} Labeled Data Ratio : {}\".format(opt.split, opt.ratio))\n",
        "        train_loader_1, train_loader_2, unlabeled_train_loader, val_loader = image_loader(image_root, gt_root,val_img_root,val_gt_root,\n",
        "                                                                                          opt.batchsize, opt.trainsize,\n",
        "                                                                                          opt.split, opt.ratio)\n",
        "        self.logger.info(\n",
        "            \"train_loader_1 {} train_loader_2 {} unlabeled_train_loader {} val_loader {}\".format(len(train_loader_1),\n",
        "                                                                                                 len(train_loader_2),\n",
        "                                                                                                 len(unlabeled_train_loader),\n",
        "                                                                                                 len(val_loader)))\n",
        "        print(\"Let's go!\")\n",
        "        for epoch in range(1, opt.epoch):\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_dice_val_1 = 0.0\n",
        "            running_dice_val_2 = 0.0\n",
        "            \n",
        "\n",
        "            for i, data in enumerate(zip(train_loader_1, train_loader_2, unlabeled_train_loader)):\n",
        "\n",
        "                inputs_S1, labels_S1 = data[0][0], data[0][1]\n",
        "                inputs_S2, labels_S2 = data[1][0], data[1][1]\n",
        "                inputs_U, labels_U = data[2][0], data[2][1]\n",
        "\n",
        "                inputs_S1, labels_S1 = Variable(inputs_S1), Variable(labels_S1)\n",
        "                inputs_S1, labels_S1 = inputs_S1.cuda(), labels_S1.cuda()\n",
        "                inputs_S2, labels_S2 = Variable(inputs_S2), Variable(labels_S2)\n",
        "                inputs_S2, labels_S2 = inputs_S2.cuda(), labels_S2.cuda()\n",
        "                inputs_U = Variable(inputs_U)\n",
        "                inputs_U = inputs_U.cuda()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                prediction_1 = self.model_1(inputs_S1)\n",
        "                prediction_1_1 = torch.sigmoid(prediction_1)\n",
        "\n",
        "                feat_1 = self.model_1(inputs_U)\n",
        "                u_prediction_1 = torch.sigmoid(feat_1)\n",
        "                 self.projector_1.cuda()\n",
        "                self.projector_2.cuda()\n",
        "                self.classifier_1.cuda()\n",
        "                self.classifier_2.cuda()\n",
        "                feat_q = self.projector_1(feat_1)\n",
        "                feat_k = self.projector_2(feat_2)\n",
        "                feat_l_q = self.classifier_1(prediction_1)\n",
        "                feat_l_k = self.classifier_2(prediction_2)\n",
        "                Loss_sup = loss_sup(prediction_1_1, prediction_2_2, labels_S1, labels_S2)\n",
        "                Loss_diff = loss_diff(u_prediction_1, u_prediction_2, opt.batchsize)\n",
        "                Loss_contrast = pixel_wise_contrastive_loss_criter(feat_q,feat_k)\n",
        "                Loss_contrast_2 = contrastive_loss_sup_criter(feat_l_q,feat_l_k)\n",
        "                \n",
        "\n",
        "                seg_loss = 0.25*Loss_sup +0.25*Loss_diff +0.25*Loss_contrast+0.25*Loss_contrast_2\n",
        "                \n",
        "                seg_loss.backward()\n",
        "                running_loss += seg_loss.item()\n",
        "                optimizer.step()\n",
        "                \n",
        "                adjust_lr(optimizer, opt.lr, epoch, opt.epoch)\n",
        "                \n",
        "                    \n",
        "\n",
        "\n",
        "            epoch_loss = running_loss / (len(train_loader_1) + len(train_loader_2))\n",
        "            self.logger.info('{} Epoch [{:03d}/{:03d}], total_loss : {:.4f}'.\n",
        "                             format(datetime.now(), epoch, opt.epoch, epoch_loss))\n",
        "\n",
        "            self.logger.info('Train loss: {}'.format(epoch_loss))\n",
        "            self.writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
        "\n",
        "            for i, pack in enumerate(val_loader, start=1):\n",
        "                with torch.no_grad():\n",
        "                    images, gts = pack\n",
        "                    images = Variable(images)\n",
        "                    gts = Variable(gts)\n",
        "                    images = images.cuda()\n",
        "                    gts = gts.cuda()\n",
        "\n",
        "                    prediction_1 = self.model_1(images)\n",
        "                    prediction_1 = torch.sigmoid(prediction_1)\n",
        "\n",
        "                    prediction_2 = self.model_2(images)\n",
        "                    prediction_2 = torch.sigmoid(prediction_2)\n",
        "\n",
        "                dice_coe_1 = dice_coef(prediction_1, gts)\n",
        "                running_dice_val_1 += dice_coe_1\n",
        "                dice_coe_2 = dice_coef(prediction_2, gts)\n",
        "                running_dice_val_2 += dice_coe_2\n",
        "\n",
        "            epoch_dice_val_1 = running_dice_val_1 / len(val_loader)\n",
        "\n",
        "            self.logger.info('Validation dice coeff model 1: {}'.format(epoch_dice_val_1))\n",
        "            self.writer.add_scalar('Validation_1/DSC', epoch_dice_val_1, epoch)\n",
        "\n",
        "            epoch_dice_val_2 = running_dice_val_2 / len(val_loader)\n",
        "\n",
        "            self.logger.info('Validation dice coeff model 1: {}'.format(epoch_dice_val_2))\n",
        "            self.writer.add_scalar('Validation_1/DSC', epoch_dice_val_2, epoch)\n",
        "\n",
        "            mdice_coeff_1 = epoch_dice_val_1\n",
        "            mdice_coeff_2 = epoch_dice_val_2\n",
        "\n",
        "            if self.best_dice_coeff_1 < mdice_coeff_1:\n",
        "                self.best_dice_coeff_1 = mdice_coeff_1\n",
        "                self.save_best_model_1 = True\n",
        "\n",
        "                if not os.path.exists(self.image_save_path_1):\n",
        "                    os.makedirs(self.image_save_path_1)\n",
        "\n",
        "                copy_tree(self.image_save_path_1, self.save_path + '/best_model_predictions_1')\n",
        "                self.patience = 0\n",
        "            else:\n",
        "                self.save_best_model_1 = False\n",
        "                self.patience += 1\n",
        "\n",
        "            if self.best_dice_coeff_2 < mdice_coeff_2:\n",
        "                self.best_dice_coeff_2 = mdice_coeff_2\n",
        "                self.save_best_model_2 = True\n",
        "\n",
        "                if not os.path.exists(self.image_save_path_2):\n",
        "                    os.makedirs(self.image_save_path_2)\n",
        "\n",
        "                copy_tree(self.image_save_path_2, self.save_path + '/best_model_predictions_2')\n",
        "                self.patience = 0\n",
        "            else:\n",
        "                self.save_best_model_2 = False\n",
        "                self.patience += 1\n",
        "\n",
        "            Checkpoints_Path = self.save_path + '/Checkpoints'\n",
        "\n",
        "            if not os.path.exists(Checkpoints_Path):\n",
        "                os.makedirs(Checkpoints_Path)\n",
        "\n",
        "            if self.save_best_model_1:\n",
        "                torch.save(self.model_1.state_dict(), Checkpoints_Path + '/Model_1.pth')\n",
        "            if self.save_best_model_2:\n",
        "                torch.save(self.model_2.state_dict(), Checkpoints_Path + '/Model_2.pth')\n",
        "\n",
        "            self.logger.info(\n",
        "                'current best dice coef model 1 {}, model 2 {}'.format(self.best_dice_coeff_1, self.best_dice_coeff_2))\n",
        "            self.logger.info('current patience :{}'.format(self.patience))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_network = Network()\n",
        "    train_network.run()"
      ],
      "metadata": {
        "id": "4eUzIQRrZJFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score, mean_absolute_error\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "from utils import get_logger, create_dir\n",
        "from model.pretrained_unet import preUnet\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--batchsize', type=int, default=1, help='training batch size')\n",
        "parser.add_argument('--trainsize', type=int, default=(512,288), help='training dataset size')\n",
        "parser.add_argument('--dataset', type=str, default='kvasir', help='dataset name')\n",
        "parser.add_argument('--threshold', type=float, default=0.5, help='threshold')\n",
        "opt = parser.parse_args()\n",
        "\n",
        "\n",
        "class Test(object):\n",
        "    def __init__(self):\n",
        "        self._init_configure()\n",
        "        self._init_logger()\n",
        "        self.model_1 = preUnet()\n",
        "        self.model_2 = preUnet()\n",
        "\n",
        "    def _init_configure(self):\n",
        "        with open('configs/config.yml') as fp:\n",
        "            self.cfg = yaml.safe_load(fp)\n",
        "\n",
        "    def _init_logger(self):\n",
        "\n",
        "        log_dir = 'logs/' + opt.dataset + '/test'\n",
        "\n",
        "        self.logger = get_logger(log_dir)\n",
        "        print('RUNDIR: {}'.format(log_dir))\n",
        "\n",
        "        self.save_path = log_dir\n",
        "        self.image_save_path_1 = log_dir + \"/saved_images_1\"\n",
        "        create_dir(self.image_save_path_1)\n",
        "        self.image_save_path_2 = log_dir + \"/saved_images_2\"\n",
        "        create_dir(self.image_save_path_2)\n",
        "\n",
        "        self.model_1_load_path = 'logs/' + opt.dataset + '/content/gdrive/MyDrive/dataset'\n",
        "\n",
        "    def visualize_val_input(self, var_map, i):\n",
        "        count = i\n",
        "        im = transforms.ToPILImage()(var_map.squeeze_(0).detach().cpu()).convert(\"RGB\")\n",
        "        name = '{:02d}_input.png'.format(count)\n",
        "        imageio.imwrite(self.image_save_path_1 + \"/val_\" + name, im)\n",
        "\n",
        "    def visualize_gt(self, var_map, i):\n",
        "        count = i\n",
        "        for kk in range(var_map.shape[0]):\n",
        "            pred_edge_kk = var_map[kk, :, :, :]\n",
        "            pred_edge_kk = pred_edge_kk.detach().cpu().numpy().squeeze()\n",
        "            pred_edge_kk *= 255.0\n",
        "            pred_edge_kk = pred_edge_kk.astype(np.uint8)\n",
        "            name = '{:02d}_gt.png'.format(count)\n",
        "            imageio.imwrite(self.image_save_path_1 + \"/val_\" + name, pred_edge_kk)\n",
        "            imageio.imwrite(self.image_save_path_2 + \"/val_\" + name, pred_edge_kk)\n",
        "\n",
        "    def visualize_prediction1(self, var_map, i):\n",
        "        count = i\n",
        "        for kk in range(var_map.shape[0]):\n",
        "            pred_edge_kk = var_map[kk, :, :, :]\n",
        "            pred_edge_kk = pred_edge_kk.detach().cpu().numpy().squeeze()\n",
        "            pred_edge_kk *= 255.0\n",
        "            pred_edge_kk = pred_edge_kk.astype(np.uint8)\n",
        "            name = '{:02d}_pred_1.png'.format(count)\n",
        "            imageio.imwrite(self.image_save_path_1 + \"/val_\" + name, pred_edge_kk)\n",
        "\n",
        "    def visualize_prediction2(self, var_map, i):\n",
        "        count = i\n",
        "        for kk in range(var_map.shape[0]):\n",
        "            pred_edge_kk = var_map[kk, :, :, :]\n",
        "            pred_edge_kk = pred_edge_kk.detach().cpu().numpy().squeeze()\n",
        "            pred_edge_kk *= 255.0\n",
        "            pred_edge_kk = pred_edge_kk.astype(np.uint8)\n",
        "            name = '{:02d}_pred_2.png'.format(count)\n",
        "            imageio.imwrite(self.image_save_path_2 + \"/val_\" + name, pred_edge_kk)\n",
        "\n",
        "    def visualize_uncertainity(self, var_map, i):\n",
        "        count = i\n",
        "        for kk in range(var_map.shape[0]):\n",
        "            pred_edge_kk = var_map[kk, :, :, :]\n",
        "            pred_edge_kk = pred_edge_kk.detach().cpu().numpy().squeeze()\n",
        "            pred_edge_kk *= 255.0\n",
        "            pred_edge_kk = pred_edge_kk.astype(np.uint8)\n",
        "            name = '{:02d}_pred.png'.format(count)\n",
        "            imageio.imwrite(self.image_save_path_1 + \"/uncertainity_\" + name, pred_edge_kk)\n",
        "\n",
        "    def evaluate_model_1(self, image_dir):\n",
        "\n",
        "        target_list = np.array([])\n",
        "        output_list = np.array([])\n",
        "        output_pred_list = np.array([])\n",
        "        test_dir = image_dir\n",
        "        self.logger.info(test_dir)\n",
        "\n",
        "        pred_files = glob.glob(test_dir + 'val_*_pred_1.png')\n",
        "        gt_files = glob.glob(test_dir + 'val_*_gt.png')\n",
        "\n",
        "        for file in pred_files:\n",
        "            image = Image.open(file)\n",
        "            output = np.asarray(image)\n",
        "            output = output.flatten() / 255\n",
        "            output_pred = (output > opt.threshold)\n",
        "            output_list = np.concatenate((output_list, output), axis=None)\n",
        "            output_pred_list = np.concatenate((output_pred_list, output_pred), axis=None)\n",
        "\n",
        "        for file in gt_files:\n",
        "            image = Image.open(file)\n",
        "            target = np.asarray(image)\n",
        "            target = target.flatten() / 255\n",
        "            target = (target > opt.threshold)\n",
        "            target_list = np.concatenate((target_list, target), axis=None)\n",
        "\n",
        "        # F1 score\n",
        "        F1_score = f1_score(target_list, output_pred_list)\n",
        "        self.logger.info(\"Model 1 F1 score : {} \".format(F1_score))\n",
        "\n",
        "        # Mean Absolute Error\n",
        "        mae = mean_absolute_error(target_list, output_pred_list)\n",
        "        self.logger.info(\"Model 1 MAE : {} \".format(mae))\n",
        "\n",
        "    def evaluate_model_2(self, image_dir):\n",
        "\n",
        "        target_list = np.array([])\n",
        "        output_list = np.array([])\n",
        "        output_pred_list = np.array([])\n",
        "        test_dir = image_dir\n",
        "        self.logger.info(test_dir)\n",
        "\n",
        "        pred_files = glob.glob(test_dir + 'val_*_pred_2.png')\n",
        "        gt_files = glob.glob(test_dir + 'val_*_gt.png')\n",
        "\n",
        "        for file in pred_files:\n",
        "            image = Image.open(file)\n",
        "            output = np.asarray(image)\n",
        "            output = output.flatten() / 255\n",
        "            output_pred = (output > opt.threshold)\n",
        "            output_list = np.concatenate((output_list, output), axis=None)\n",
        "            output_pred_list = np.concatenate((output_pred_list, output_pred), axis=None)\n",
        "\n",
        "        for file in gt_files:\n",
        "            image = Image.open(file)\n",
        "            target = np.asarray(image)\n",
        "            target = target.flatten() / 255\n",
        "            target = (target > opt.threshold)\n",
        "            target_list = np.concatenate((target_list, target), axis=None)\n",
        "        F1_score = f1_score(target_list, output_pred_list)\n",
        "        self.logger.info(\"Model 2 F1 score : {} \".format(F1_score))\n",
        "        mae = mean_absolute_error(target_list, output_pred_list)\n",
        "        self.logger.info(\"Model 2 MAE : {} \".format(mae))\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        self.model_1.load_state_dict(torch.load(self.model_1_load_path))\n",
        "        self.model_1.cuda()\n",
        "\n",
        "        self.model_2.load_state_dict(torch.load(self.model_2_load_path))\n",
        "        self.model_2.cuda()\n",
        "\n",
        "\n",
        "        \n",
        "        image_root = './data/'+ opt.dataset +'/train/image/'\n",
        "        gt_root = './data/'+ opt.dataset +'/train/mask/'\n",
        "        val_img_root = './data/'+ opt.dataset +'/test/image/'\n",
        "        val_gt_root = './data/'+ opt.dataset +'/test/mask/'\n",
        "\n",
        "        _, _, _, val_loader = image_loader(image_root, gt_root,val_img_root,val_gt_root, opt.batchsize, opt.trainsize)\n",
        "\n",
        "        for i, pack in enumerate(val_loader, start=1):\n",
        "            with torch.no_grad():\n",
        "                images, gts = pack\n",
        "                images = Variable(images)\n",
        "                gts = Variable(gts)\n",
        "                images = images.cuda()\n",
        "                gts = gts.cuda()\n",
        "\n",
        "                feat_map_1 = self.model_1(images)\n",
        "                prediction1 = torch.sigmoid(feat_map_1)\n",
        "\n",
        "                feat_map_2 = self.model_2(images)\n",
        "                prediction2 = torch.sigmoid(feat_map_2)\n",
        "\n",
        "            self.visualize_val_input(images, i)\n",
        "            self.visualize_gt(gts, i)\n",
        "            self.visualize_prediction1(prediction1, i)\n",
        "            self.visualize_prediction2(prediction2, i)\n",
        "\n",
        "        self.evaluate_model_1('logs/kvasir/test/saved_images_1/')\n",
        "        self.evaluate_model_2('logs/kvasir/test/saved_images_2/')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Test_network = Test()\n",
        "    Test_network.run()"
      ],
      "metadata": {
        "id": "zRMfPqkQZN1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef(output, target):\n",
        "    smooth = 1e-5\n",
        "\n",
        "    output = output.view(-1).data.cpu().numpy()\n",
        "    target = target.view(-1).data.cpu().numpy()\n",
        "    intersection = (output * target).sum()\n",
        "\n",
        "    return (2. * intersection + smooth) / (output.sum() + target.sum() + smooth)\n"
      ],
      "metadata": {
        "id": "-ZBauyKwZT7U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "BCE = torch.nn.BCELoss()\n",
        "\n",
        "def weighted_loss(pred, mask):\n",
        "    \n",
        "    weit = 1 + 5*torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
        "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
        "    wbce = (weit*wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
        "    inter = ((pred * mask)*weit).sum(dim=(2, 3))\n",
        "    union = ((pred + mask)*weit).sum(dim=(2, 3))\n",
        "    wiou = 1 - (inter + 1)/(union - inter+1)\n",
        "    \n",
        "    return (wbce + wiou).mean()\n",
        "\n",
        "\n",
        "\n",
        "def calc_loss(pred, target, bce_weight=0.5):\n",
        "    bce = weighted_loss(pred, target)\n",
        "    return bce\n",
        "\n",
        "\n",
        "def loss_sup(logit_S1, logit_S2, labels_S1, labels_S2):\n",
        "    loss1 = calc_loss(logit_S1, labels_S1)\n",
        "    loss2 = calc_loss(logit_S2, labels_S2)\n",
        "\n",
        "    return loss1 + loss2\n",
        "\n",
        "\n",
        "\n",
        "def loss_diff(u_prediction_1, u_prediction_2, batch_size):\n",
        "    a = weighted_loss(u_prediction_1, Variable(u_prediction_2, requires_grad=False))\n",
        "    a = a.item()\n",
        "\n",
        "    b = weighted_loss(u_prediction_2, Variable(u_prediction_1, requires_grad=False))\n",
        "    b = b.item()\n",
        "\n",
        "    loss_diff_avg = (a + b)\n",
        "    return loss_diff_avg / batch_size\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EvkoS3pqZg6J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pdb\n",
        "import math\n",
        "import torchvision\n",
        " \n",
        " \n",
        "class Grid(object):\n",
        "    def __init__(self, d1, d2, rotate=1, ratio=0.5, mode=0, prob=1.):\n",
        "        self.d1 = d1\n",
        "        self.d2 = d2\n",
        "        self.rotate = rotate\n",
        "        self.ratio = ratio\n",
        "        self.mode = mode\n",
        "        self.st_prob = self.prob = prob\n",
        " \n",
        "    def set_prob(self, epoch, max_epoch):\n",
        "        self.prob = self.st_prob * min(1, epoch / max_epoch)\n",
        " \n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() > self.prob:\n",
        "            return img\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "        hh = math.ceil((math.sqrt(h * h + w * w)))\n",
        " \n",
        "        d = np.random.randint(self.d1, self.d2)\n",
        "        self.l = math.ceil(d * self.ratio)\n",
        " \n",
        "        mask = np.ones((hh, hh), np.float32)\n",
        "        st_h = np.random.randint(d)\n",
        "        st_w = np.random.randint(d)\n",
        "        for i in range(-1, hh // d + 1):\n",
        "            s = d * i + st_h\n",
        "            t = s + self.l\n",
        "            s = max(min(s, hh), 0)\n",
        "            t = max(min(t, hh), 0)\n",
        "            mask[s:t, :] *= 0\n",
        " \n",
        "        for i in range(-1, hh // d + 1):\n",
        "            s = d * i + st_w\n",
        "            t = s + self.l\n",
        "            s = max(min(s, hh), 0)\n",
        "            t = max(min(t, hh), 0)\n",
        "            mask[:, s:t] *= 0\n",
        " \n",
        "        r = np.random.randint(self.rotate)\n",
        "        mask = Image.fromarray(np.uint8(mask))\n",
        "        mask = mask.rotate(r)\n",
        "        mask = np.asarray(mask)\n",
        "        mask = mask[(hh - h) // 2:(hh - h) // 2 + h, (hh - w) // 2:(hh - w) // 2 + w]\n",
        " \n",
        "        mask = torch.from_numpy(mask).float()\n",
        "        if self.mode == 1:\n",
        "            mask = 1 - mask\n",
        " \n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        " \n",
        "        return img\n",
        " \n",
        " \n",
        "class GridMask(nn.Module):\n",
        "    def __init__(self, d1=20, d2=80, rotate=90, ratio=0.4, mode=1, prob=0.8):\n",
        "        super(GridMask, self).__init__()\n",
        "        self.rotate = rotate\n",
        "        self.ratio = ratio\n",
        "        self.mode = mode\n",
        "        self.st_prob = prob\n",
        "        self.grid = Grid(d1, d2, rotate, ratio, mode, prob)\n",
        " \n",
        "    def set_prob(self, epoch, max_epoch):\n",
        "        self.grid.set_prob(epoch, max_epoch)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        if not self.training:\n",
        "            return x\n",
        "        \n",
        "        return self.grid(x)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    import cv2\n",
        "    from torchvision import transforms\n",
        "     \n",
        "    img = cv2.imread('./data/kvasir/train/image/ckcu8xad600033b5yc78xfyjx.jpg')\n",
        "    img = torchvision.transforms.ToTensor()(img)\n",
        "    grid_mask = GridMask()\n",
        "    img = grid_mask(img)\n",
        "     \n",
        "    img = img.mul(255).byte()\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    cv2.imwrite('gridmask.jpg', img)"
      ],
      "metadata": {
        "id": "gfPlJzF4ZkZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "from torchvision.transforms import transforms\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "class ObjDataset(data.Dataset):\n",
        "    def __init__(self, images, gts, trainsize, mode):\n",
        "        self.trainsize = trainsize\n",
        "        self.images = images\n",
        "        self.mode = mode\n",
        "        self.gts = gts\n",
        "        self.images = sorted(self.images)\n",
        "        self.gts = sorted(self.gts)\n",
        "        self.filter_files()\n",
        "        self.size = len(self.images)\n",
        "        self.gridmask = GridMask()\n",
        "        self.img_transform_w = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        \n",
        "        self.gt_transform_w = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize)),\n",
        "            transforms.ToTensor()])\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.img_transform_s = transforms.Compose([\n",
        "            transforms.RandomRotation(90),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomAffine(degrees = 90,translate=(0.5,0.5),shear=30),\n",
        "            transforms.ColorJitter(hue = 0.5),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.GaussianBlur(3),\n",
        "            transforms.Resize((self.trainsize)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "        \n",
        "        self.gt_transform_s = transforms.Compose([\n",
        "            transforms.RandomRotation(90),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomAffine(degrees = 90,translate=(0.5,0.5),shear=30),\n",
        "            transforms.Resize((self.trainsize)),\n",
        "            transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.rgb_loader(self.images[index])\n",
        "        gt = self.binary_loader(self.gts[index])\n",
        "        \n",
        "        seed =  np.random.randint(2147483647)\n",
        "        \n",
        "\n",
        "        \n",
        "        if self.mode == 'weak':\n",
        "            \n",
        "            image = self.img_transform_w(image)\n",
        "            gt = self.gt_transform_w(gt)\n",
        "\n",
        "        if self.mode =='strong':\n",
        "            torch.manual_seed(seed)\n",
        "            image = self.img_transform_s(image)\n",
        "            torch.manual_seed(seed)\n",
        "            gt = self.gt_transform_s(gt)\n",
        "            image = self.gridmask(image)\n",
        "            \n",
        "\n",
        "        return image, gt\n",
        "\n",
        "    def filter_files(self):\n",
        "        assert len(self.images) == len(self.gts)\n",
        "        images = []\n",
        "        gts = []\n",
        "        for img_path, gt_path in zip(self.images, self.gts):\n",
        "            img = Image.open(img_path)\n",
        "            gt = Image.open(gt_path)\n",
        "            if img.size == gt.size:\n",
        "                images.append(img_path)\n",
        "                gts.append(gt_path)\n",
        "        self.images = images\n",
        "        self.gts = gts\n",
        "\n",
        "    def rgb_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "    def binary_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('L')\n",
        "\n",
        "    def resize(self, img, gt):\n",
        "        assert img.size == gt.size\n",
        "        w, h = img.size\n",
        "        if h < self.trainsize or w < self.trainsize:\n",
        "            h = max(h, self.trainsize)\n",
        "            w = max(w, self.trainsize)\n",
        "            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST)\n",
        "        else:\n",
        "            return img, gt\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "class ValObjDataset(data.Dataset):\n",
        "    def __init__(self, images, gts, trainsize):\n",
        "        self.trainsize = trainsize\n",
        "        self.images = images\n",
        "        self.gts = gts\n",
        "        self.images = sorted(self.images)\n",
        "        self.gts = sorted(self.gts)\n",
        "        self.filter_files()\n",
        "        self.size = len(self.images)\n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize)),\n",
        "            transforms.ToTensor()])\n",
        "        self.gt_transform = transforms.Compose([\n",
        "            transforms.Resize((self.trainsize)),\n",
        "            transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.rgb_loader(self.images[index])\n",
        "        gt = self.binary_loader(self.gts[index])\n",
        "\n",
        "        image = self.img_transform(image)\n",
        "        gt = self.gt_transform(gt)\n",
        "\n",
        "        return image, gt\n",
        "\n",
        "    def filter_files(self):\n",
        "        assert len(self.images) == len(self.gts)\n",
        "        images = []\n",
        "        gts = []\n",
        "        for img_path, gt_path in zip(self.images, self.gts):\n",
        "            img = Image.open(img_path)\n",
        "            gt = Image.open(gt_path)\n",
        "            if img.size == gt.size:\n",
        "                images.append(img_path)\n",
        "                gts.append(gt_path)\n",
        "        self.images = images\n",
        "        self.gts = gts\n",
        "\n",
        "    def rgb_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "    def binary_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            # return img.convert('1')\n",
        "            return img.convert('L')\n",
        "\n",
        "    def resize(self, img, gt):\n",
        "        assert img.size == gt.size\n",
        "        w, h = img.size\n",
        "        if h < 256 or w < 256:\n",
        "            h = max(h, 256)\n",
        "            w = max(w, 256)\n",
        "            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST)\n",
        "        else:\n",
        "            return img, gt\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "def image_loader(image_root, gt_root,val_img_root,val_gt_root, batch_size, image_size, split=1, labeled_ratio=0.05,mode='weak_1'):\n",
        "    images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "    gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "    \n",
        "    val_img = [val_img_root+ f for f in os.listdir(val_img_root) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "    val_label = [val_gt_root+ f for f in os.listdir(val_gt_root) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "    \n",
        "    train_images = images[0:int(len(images) * split)]\n",
        "    val_images = val_img[0:int(len(val_img) * split)] \n",
        "    val_gts = val_label[0:int(len(val_label) * split)]\n",
        "    train_gts = gts[0:int(len(images) * split)]\n",
        "\n",
        "\n",
        "    labeled_train_images = train_images[0:int(len(train_images) * labeled_ratio)] \n",
        "    labeled_train_images_1 = labeled_train_images[0:int(len(labeled_train_images) * 0.5)] \n",
        "    labeled_train_images_2 = labeled_train_images[int(len(labeled_train_images) * 0.5):]\n",
        "    unlabeled_train_images = train_images[int(len(train_images) * labeled_ratio):] \n",
        "    labeled_train_gts = train_gts[0:int(len(train_gts) * labeled_ratio)]\n",
        "    labeled_train_gts_1 = labeled_train_gts[0:int(len(labeled_train_gts) * 0.5)]\n",
        "    labeled_train_gts_2 = labeled_train_gts[int(len(labeled_train_gts) * 0.5):]\n",
        "    unlabeled_train_gts = train_gts[int(len(train_gts) * labeled_ratio):]\n",
        "\n",
        "    labeled_train_dataset_1 = ObjDataset(labeled_train_images_1, labeled_train_gts_1, image_size,mode='weak')\n",
        "    labeled_train_dataset_2 = ObjDataset(labeled_train_images_2, labeled_train_gts_2, image_size,mode='weak')\n",
        "    unlabeled_train_dataset = ObjDataset(unlabeled_train_images, unlabeled_train_gts, image_size,mode='strong')\n",
        "    val_dataset = ValObjDataset(val_images, val_gts, image_size)\n",
        "\n",
        "    labeled_data_loader_1 = data.DataLoader(dataset=labeled_train_dataset_1,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=1,\n",
        "                                  pin_memory=True,\n",
        "                                  shuffle=True)\n",
        "\n",
        "    labeled_data_loader_2 = data.DataLoader(dataset=labeled_train_dataset_2,\n",
        "                                            batch_size=batch_size,\n",
        "                                            num_workers=1,\n",
        "                                            pin_memory=True,\n",
        "                                            shuffle=True)\n",
        "\n",
        "    unlabeled_data_loader = data.DataLoader(dataset=unlabeled_train_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          num_workers=1,\n",
        "                                          pin_memory=True,\n",
        "                                          shuffle=True)\n",
        "\n",
        "    val_loader = data.DataLoader(dataset=val_dataset,\n",
        "                                 batch_size=batch_size,\n",
        "                                 num_workers=1,\n",
        "                                 pin_memory=True,\n",
        "                                 shuffle=False)\n",
        "\n",
        "    return labeled_data_loader_1, labeled_data_loader_2, unlabeled_data_loader, val_loader"
      ],
      "metadata": {
        "id": "aojIm2ErZo8g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ConLoss(torch.nn.Module):\n",
        "    def __init__(self, temperature=0.07, base_temperature=0.07):\n",
        "        \"\"\"\n",
        "        Contrastive Learning for Unpaired Image-to-Image Translation\n",
        "        models/patchnce.py\n",
        "        \"\"\"\n",
        "        super(ConLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.base_temperature = base_temperature\n",
        "        self.nce_includes_all_negatives_from_minibatch = False\n",
        "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "        self.mask_dtype = torch.bool\n",
        "\n",
        "    def forward(self, feat_q, feat_k):\n",
        "        assert feat_q.size() == feat_k.size(), (feat_q.size(), feat_k.size())\n",
        "        batch_size = feat_q.shape[0]\n",
        "        dim = feat_q.shape[1]\n",
        "        width = feat_q.shape[2]\n",
        "        feat_q = feat_q.view(batch_size, dim, -1).permute(0, 2, 1)\n",
        "        feat_k = feat_k.view(batch_size, dim, -1).permute(0, 2, 1)\n",
        "        feat_q = F.normalize(feat_q, dim=-1, p=1)\n",
        "        feat_k = F.normalize(feat_k, dim=-1, p=1)\n",
        "        feat_k = feat_k.detach()\n",
        "        l_pos = torch.bmm(feat_q.reshape(-1, 1, dim), feat_k.reshape(-1, dim, 1))\n",
        "        l_pos = l_pos.view(-1, 1)\n",
        "        if self.nce_includes_all_negatives_from_minibatch:\n",
        "            batch_dim_for_bmm = 1\n",
        "        else:\n",
        "            batch_dim_for_bmm = batch_size\n",
        "        feat_q = feat_q.reshape(batch_dim_for_bmm, -1, dim)\n",
        "        feat_k = feat_k.reshape(batch_dim_for_bmm, -1, dim)\n",
        "        npatches = feat_q.size(1)\n",
        "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
        "\n",
        "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
        "\n",
        "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
        "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
        "\n",
        "        out = torch.cat((l_pos, l_neg), dim=1) / self.temperature\n",
        "\n",
        "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
        "                                                        device=feat_q.device))\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "class contrastive_loss_sup(torch.nn.Module):\n",
        "    def __init__(self, temperature=0.07, base_temperature=0.07):\n",
        "      super(contrastive_loss_sup, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.base_temperature = base_temperature\n",
        "        self.nce_includes_all_negatives_from_minibatch = False\n",
        "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "        self.mask_dtype = torch.bool\n",
        "\n",
        "    def forward(self, feat_q, feat_k):\n",
        "        assert feat_q.size() == feat_k.size(), (feat_q.size(), feat_k.size())\n",
        "        batch_size = feat_q.shape[0]\n",
        "        dim = feat_q.shape[1]\n",
        "        width = feat_q.shape[2]\n",
        "        feat_q = feat_q.view(batch_size, dim, -1).permute(0, 2, 1)\n",
        "        feat_k = feat_k.view(batch_size, dim, -1).permute(0, 2, 1)\n",
        "        feat_q = F.normalize(feat_q, dim=-1, p=1)\n",
        "        feat_k = F.normalize(feat_k, dim=-1, p=1)\n",
        "        feat_k = feat_k.detach()\n",
        "        l_pos = torch.zeros((batch_size*2304,1)).cuda()\n",
        "        if self.nce_includes_all_negatives_from_minibatch:\n",
        "        else:\n",
        "            batch_dim_for_bmm = batch_size\n",
        "        feat_q = feat_q.reshape(batch_dim_for_bmm, -1, dim)\n",
        "        feat_k = feat_k.reshape(batch_dim_for_bmm, -1, dim)\n",
        "        npatches = feat_q.size(1)\n",
        "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
        "\n",
        "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
        "\n",
        "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
        "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
        "\n",
        "        out = torch.cat((l_pos, l_neg), dim=1) / self.temperature\n",
        "\n",
        "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
        "                                                        device=feat_q.device))\n",
        "\n",
        "        return loss  \n",
        "    \n",
        "  \n",
        "    "
      ],
      "metadata": {
        "id": "XjdVucc-aLjJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "__all__ = ['Res2Net', 'res2net50_v1b', 'res2net101_v1b', 'res2net50_v1b_26w_4s']\n",
        "\n",
        "model_urls = {\n",
        "    'res2net50_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_v1b_26w_4s-3cf99910.pth',\n",
        "    'res2net101_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net101_v1b_26w_4s-0812c246.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Bottle2neck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale=4, stype='normal'):\n",
        "        \n",
        "        super(Bottle2neck, self).__init__()\n",
        "\n",
        "        width = int(math.floor(planes * (baseWidth / 64.0)))\n",
        "        self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(width * scale)\n",
        "\n",
        "        if scale == 1:\n",
        "            self.nums = 1\n",
        "        else:\n",
        "            self.nums = scale - 1\n",
        "        if stype == 'stage':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n",
        "        convs = []\n",
        "        bns = []\n",
        "        for i in range(self.nums):\n",
        "            convs.append(nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "            bns.append(nn.BatchNorm2d(width))\n",
        "        self.convs = nn.ModuleList(convs)\n",
        "        self.bns = nn.ModuleList(bns)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stype = stype\n",
        "        self.scale = scale\n",
        "        self.width = width\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        spx = torch.split(out, self.width, 1)\n",
        "        for i in range(self.nums):\n",
        "            if i == 0 or self.stype == 'stage':\n",
        "                sp = spx[i]\n",
        "            else:\n",
        "                sp = sp + spx[i]\n",
        "            sp = self.convs[i](sp)\n",
        "            sp = self.relu(self.bns[i](sp))\n",
        "            if i == 0:\n",
        "                out = sp\n",
        "            else:\n",
        "                out = torch.cat((out, sp), 1)\n",
        "        if self.scale != 1 and self.stype == 'normal':\n",
        "            out = torch.cat((out, spx[self.nums]), 1)\n",
        "        elif self.scale != 1 and self.stype == 'stage':\n",
        "            out = torch.cat((out, self.pool(spx[self.nums])), 1)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Res2Net(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, baseWidth=26, scale=4, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(Res2Net, self).__init__()\n",
        "        self.baseWidth = baseWidth\n",
        "        self.scale = scale\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.AvgPool2d(kernel_size=stride, stride=stride,\n",
        "                             ceil_mode=True, count_include_pad=False),\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                            stype='stage', baseWidth=self.baseWidth, scale=self.scale))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, baseWidth=self.baseWidth, scale=self.scale))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def res2net50_v1b(pretrained=False, **kwargs):\n",
        "    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def res2net101_v1b(pretrained=False, **kwargs):\n",
        "     model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def res2net50_v1b_26w_4s(pretrained=False, **kwargs):\n",
        "     model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n",
        "    if pretrained:\n",
        "        model_state = torch.load('D:/HarDNet-MSEG-master/res2net50_v1b_26w_4s-3cf99910.pth')\n",
        "        model.load_state_dict(model_state)\n",
        "    return model\n",
        "\n",
        "\n",
        "def res2net101_v1b_26w_4s(pretrained=False, **kwargs):\n",
        "   model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def res2net152_v1b_26w_4s(pretrained=False, **kwargs):\n",
        "     model = Res2Net(Bottle2neck, [3, 8, 36, 3], baseWidth=26, scale=4, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['res2net152_v1b_26w_4s']))\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = torch.rand(1, 3, 224, 224).cuda(0)\n",
        "    model = res2net50_v1b_26w_4s(pretrained=True)\n",
        "    model = model.cuda(0)\n",
        "    print(model(images).size())"
      ],
      "metadata": {
        "id": "i8UHWnypjCR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}